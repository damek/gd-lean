/-
This file was generated by Aristotle.

Lean version: leanprover/lean4:v4.24.0
Mathlib version: f897ebcf72cd16f89ab4577d0c826cd14afaafc7
This project request had uuid: e623ec93-eb6e-4db9-a318-9702e060e589
Provenance: raw Aristotle output is at `e623ec93-eb6e-4db9-a318-9702e060e589-output.lean`; Codex cleanup diff is at `Gd/Lean/Aristotle/codex_cleanup.diff`.

To cite Aristotle, tag @Aristotle-Harmonic on GitHub PRs/issues, and add as co-author to commits:
Co-authored-by: Aristotle (Harmonic) <aristotle-harmonic@harmonic.fun>
-/

/-
This module formalizes the convergence analysis of Gradient Descent for convex, L-smooth functions.
It includes the Descent Lemma, the one-step decrease property, monotonicity of the objective function,
the first-order convexity condition, a key telescoping inequality, and the final O(1/k) convergence rate theorem.
The constant in the rate is explicit: C = ||x₀ - x*||² / (2η).
-/

import Mathlib

set_option linter.mathlibStandardSet false

open scoped BigOperators
open scoped Real
open scoped Nat
open scoped Classical
open scoped Pointwise

set_option maxHeartbeats 0
set_option maxRecDepth 4000
set_option synthInstance.maxHeartbeats 20000
set_option synthInstance.maxSize 128

set_option relaxedAutoImplicit false
set_option autoImplicit false

noncomputable section

/-
If f is differentiable with L-Lipschitz gradient, then f(y) ≤ f(x) + ⟨∇f(x), y-x⟩ + (L/2)‖y-x‖².
-/
theorem descent_lemma
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  {f : E → ℝ} {x y : E} {L : NNReal}
  (hf : Differentiable ℝ f)
  (h_lip : LipschitzWith L (gradient f)) :
  f y ≤ f x + inner ℝ (gradient f x) (y - x) + (L / 2) * ‖y - x‖ ^ 2 := by
    -- Apply the fundamental theorem of calculus to rewrite the difference f(y) - f(x) as an integral.
    have int_eq : f y - f x = ∫ t in (0 : ℝ)..1, inner ℝ (gradient f (x + t • (y - x))) (y - x) := by
      rw [ intervalIntegral.integral_eq_sub_of_hasDerivAt ];
      rotate_right;
      use fun t => f ( x + t • ( y - x ) );
      · simp +decide;
      · intro t ht;
        convert HasFDerivAt.hasDerivAt ( hf.differentiableAt.hasFDerivAt.comp t ( HasFDerivAt.add ( hasFDerivAt_const _ _ ) ( HasFDerivAt.smul ( hasFDerivAt_id t ) ( hasFDerivAt_const _ _ ) ) ) ) using 1;
        simp +decide [ gradient ];
      · apply_rules [ Continuous.intervalIntegrable ];
        exact Continuous.inner ( h_lip.continuous.comp ( continuous_const.add ( continuous_id.smul continuous_const ) ) ) continuous_const;
    -- Using the Lipschitz property, we can bound the integrand $\langle \nabla f(x + t(y - x)), y - x \rangle$ by $\langle \nabla f(x), y - x \rangle + Lt \|y - x\|^2$.
    have int_bound : ∫ t in (0 : ℝ)..1, inner ℝ (gradient f (x + t • (y - x))) (y - x) ≤ ∫ t in (0 : ℝ)..1, inner ℝ (gradient f x) (y - x) + L * t * ‖y - x‖ ^ 2 := by
      refine' intervalIntegral.integral_mono_on _ _ _ _;
      · norm_num;
      · apply_rules [ Continuous.intervalIntegrable ];
        exact Continuous.inner ( h_lip.continuous.comp ( continuous_const.add ( continuous_id.smul continuous_const ) ) ) continuous_const;
      · exact Continuous.intervalIntegrable ( by continuity ) _ _;
      · -- By the properties of the inner product and the Lipschitz condition, we have:
        intro t ht
        have h_inner : inner ℝ (gradient f (x + t • (y - x))) (y - x) - inner ℝ (gradient f x) (y - x) ≤ L * t * ‖y - x‖^2 := by
          have h_inner : ‖gradient f (x + t • (y - x)) - gradient f x‖ ≤ L * t * ‖y - x‖ := by
            convert h_lip.norm_sub_le ( x + t • ( y - x ) ) x using 1 ; simp +decide [ norm_smul, abs_of_nonneg ht.1 ] ; ring_nf;
          have := abs_le.mp ( abs_real_inner_le_norm ( gradient f ( x + t • ( y - x ) ) - gradient f x ) ( y - x ) );
          rw [ inner_sub_left ] at this ; nlinarith [ norm_nonneg ( y - x ) ];
        linarith;
    norm_num [ mul_assoc ] at *; linarith;

/-
One step of gradient descent decreases the function value by at least η(1 - Lη/2)‖∇f(x)‖².
-/
theorem gd_decrease
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  {f : E → ℝ} {x : E} {L : NNReal} {η : ℝ}
  (hf : Differentiable ℝ f)
  (h_lip : LipschitzWith L (gradient f))
  (hη : 0 ≤ η) :
  f (x - η • gradient f x) ≤ f x - η * (1 - L * η / 2) * ‖gradient f x‖ ^ 2 := by
  have := @descent_lemma E _ _ _ f x ( x - η • gradient f x ) L hf h_lip;
  convert this using 1 ; simp +decide [ inner_smul_right ] ; ring_nf;
  rw [ norm_smul, Real.norm_eq_abs, abs_of_nonneg hη ] ; rw [ real_inner_self_eq_norm_sq ] ; ring_nf;

/-
If the step size is small enough (η ≤ 1/L), gradient descent is monotone decreasing.
-/
theorem gd_monotone
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  {f : E → ℝ} {x : E} {L : NNReal} {η : ℝ}
  (hf : Differentiable ℝ f)
  (h_lip : LipschitzWith L (gradient f))
  (hη_pos : 0 ≤ η)
  (hη_le : η ≤ 1 / L) :
  f (x - η • gradient f x) ≤ f x := by
  -- Apply the descent lemma with the given step size.
  have h_desc : f (x - η • gradient f x) ≤ f x - η * (1 - L * η / 2) * ‖gradient f x‖ ^ 2 := by
    convert gd_decrease hf h_lip hη_pos using 1;
  by_cases hL : L = 0 <;> simp_all +decide [ mul_div_assoc ];
  · cases le_antisymm hη_le hη_pos ; aesop;
  · exact h_desc.trans ( sub_le_self _ ( mul_nonneg ( mul_nonneg hη_pos ( sub_nonneg.2 <| by nlinarith [ inv_mul_cancel₀ ( show ( L : ℝ ) ≠ 0 by positivity ), show ( L : ℝ ) ≥ 0 by positivity ] ) ) ( sq_nonneg _ ) ) )

/-
First-order condition for convex differentiable functions: f(y) ≥ f(x) + ⟨∇f(x), y-x⟩.
-/
theorem convex_first_order_condition
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  {f : E → ℝ} {x y : E}
  (hf_conv : ConvexOn ℝ Set.univ f)
  (hf_diff : Differentiable ℝ f) :
  f x + inner ℝ (gradient f x) (y - x) ≤ f y := by
  -- Fix an arbitrary $y \in E$.
  have h_fixed : ∀ y, f y ≥ f x + inner ℝ (fderiv ℝ f x (y - x)) 1 := by
    intro y
    have h_convex : ∀ t ∈ Set.Icc (0 : ℝ) 1, f (x + t • (y - x)) ≤ (1 - t) * f x + t * f y := by
      intro t ht
      have h_convex : f (x + t • (y - x)) ≤ (1 - t) * f x + t * f y := by
        have h_convex_def : ConvexOn ℝ Set.univ f := by
          exact hf_conv
        convert h_convex_def.2 ( Set.mem_univ x ) ( Set.mem_univ y ) ( by linarith [ ht.1, ht.2 ] : 0 ≤ 1 - t ) ( by linarith [ ht.1, ht.2 ] : 0 ≤ t ) ( by linarith [ ht.1, ht.2 ] ) using 1 ; simp +decide [ add_comm, smul_sub ];
        simp +decide [ sub_smul ];
        exact congr_arg f ( by abel1 );
      exact h_convex;
    -- By definition of the derivative, we know that
    have h_deriv : Filter.Tendsto (fun t : ℝ => (f (x + t • (y - x)) - f x) / t) (nhdsWithin 0 (Set.Ioi 0)) (nhds (fderiv ℝ f x (y - x))) := by
      have h_deriv : HasDerivAt (fun t : ℝ => f (x + t • (y - x))) (fderiv ℝ f x (y - x)) 0 := by
        convert HasFDerivAt.hasDerivAt ( HasFDerivAt.comp 0 ( hf_diff.differentiableAt.hasFDerivAt ) ( HasFDerivAt.add ( hasFDerivAt_const _ _ ) ( HasFDerivAt.smul ( hasFDerivAt_id 0 ) ( hasFDerivAt_const _ _ ) ) ) ) using 1 ; norm_num;
      simpa [ div_eq_inv_mul ] using h_deriv.tendsto_slope_zero_right;
    -- By definition of the derivative, we know that for $t > 0$, $(f (x + t • (y - x)) - f x) / t \leq f y - f x$.
    have h_deriv_le : ∀ t ∈ Set.Ioo (0 : ℝ) 1, (f (x + t • (y - x)) - f x) / t ≤ f y - f x := by
      intro t ht; rw [ div_le_iff₀ ht.1 ] ; linarith [ h_convex t <| Set.Ioo_subset_Icc_self ht ] ;
    have h_deriv_le : (fderiv ℝ f x (y - x)) ≤ f y - f x := by
      exact le_of_tendsto h_deriv ( Filter.eventually_of_mem ( Ioo_mem_nhdsGT_of_mem ⟨ by norm_num, by norm_num ⟩ ) h_deriv_le );
    norm_num [ inner_self_eq_norm_sq_to_K ] at * ; linarith!;
  simp_all +decide [ gradient ]

/-
The telescoping inequality: f(x_{k+1}) - f(x^*) ≤ (1/2η)(‖x_k - x^*‖² - ‖x_{k+1} - x^*‖²).
-/
theorem gd_telescope
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  {f : E → ℝ} {x x_star : E} {L : NNReal} {η : ℝ}
  (hf_conv : ConvexOn ℝ Set.univ f)
  (hf_diff : Differentiable ℝ f)
  (h_lip : LipschitzWith L (gradient f))
  (_h_min : ∀ z, f x_star ≤ f z)
  (hη_pos : 0 < η)
  (hη_le : η ≤ 1 / L) :
  f (x - η • gradient f x) - f x_star ≤ (1 / (2 * η)) * (‖x - x_star‖ ^ 2 - ‖(x - η • gradient f x) - x_star‖ ^ 2) := by
  -- By the first-order condition for convex differentiable functions, we have $$f(x) - f(x_star) \leq ⟨∇f(x), x - x_star⟩.$$
  have h_first_order : f x - f x_star ≤ inner ℝ (gradient f x) (x - x_star) := by
    have h_first_order : f x + inner ℝ (gradient f x) (x_star - x) ≤ f x_star := by
      apply_rules [ convex_first_order_condition ];
    simp_all +decide [ inner_sub_right ];
    linarith;
  -- By the descent lemma, we have $$f(x - η∇f(x)) ≤ f(x) - η(1 - Lη/2)‖∇f(x)‖².$$
  have h_descent : f (x - η • gradient f x) ≤ f x - η * (1 - L * η / 2) * ‖gradient f x‖ ^ 2 := by
    convert gd_decrease hf_diff h_lip hη_pos.le using 1;
  -- By the properties of the inner product and the norm, we can expand and simplify the expression.
  have h_expand : ‖x - η • gradient f x - x_star‖ ^ 2 = ‖x - x_star‖ ^ 2 - 2 * η * inner ℝ (gradient f x) (x - x_star) + η ^ 2 * ‖gradient f x‖ ^ 2 := by
    simp +decide [ *, norm_sub_sq_real, inner_sub_left, inner_sub_right, inner_smul_left, inner_smul_right ] ; ring_nf;
    rw [ norm_smul, Real.norm_eq_abs, abs_of_nonneg hη_pos.le ] ; ring_nf;
    rw [ real_inner_comm ] ; ring_nf;
  by_cases hL : L = 0 <;> simp_all +decide [ div_eq_inv_mul ];
  · linarith;
  · field_simp;
    nlinarith [ mul_inv_cancel_left₀ ( show ( L : ℝ ) ≠ 0 by simpa ) η, mul_le_mul_of_nonneg_left hη_le hη_pos.le, show ( 0 : ℝ ) ≤ η ^ 2 * ‖gradient f x‖ ^ 2 by positivity ]

/-
The sequence of iterates generated by gradient descent.
-/
def gd_sequence
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  (f : E → ℝ) (x₀ : E) (η : ℝ) : ℕ → E
  | 0 => x₀
  | k + 1 => gd_sequence f x₀ η k - η • gradient f (gd_sequence f x₀ η k)

/-
Convergence rate of gradient descent for convex L-smooth functions: f(x_{k+1}) - f(x^*) ≤ ‖x_0 - x^*‖² / (2η(k+1)).
-/
theorem gd_rate
  {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ℝ E] [CompleteSpace E]
  {f : E → ℝ} {x₀ x_star : E} {L : NNReal} {η : ℝ}
  (hf_conv : ConvexOn ℝ Set.univ f)
  (hf_diff : Differentiable ℝ f)
  (h_lip : LipschitzWith L (gradient f))
  (h_min : ∀ z, f x_star ≤ f z)
  (hη_pos : 0 < η)
  (hη_le : η ≤ 1 / L)
  (k : ℕ) :
  f (gd_sequence f x₀ η (k + 1)) - f x_star ≤ ‖x₀ - x_star‖ ^ 2 / (2 * η * (k + 1)) := by
  -- By induction on $k$, we can show that $f(x_{k+1}) - f(x^*) \leq \frac{1}{k+1} \sum_{i=0}^k (f(x_{i+1}) - f(x^*))$.
  have h_ind : ∀ k, (f (gd_sequence f x₀ η (k + 1))) - f x_star ≤ (1 / (k + 1)) * ∑ i ∈ Finset.range (k + 1), (f (gd_sequence f x₀ η (i + 1)) - f x_star) := by
    field_simp;
    intro k;
    -- By induction on $k$, we can show that $f(x_{k+1}) \leq f(x_k)$.
    have h_monotone : ∀ k, f (gd_sequence f x₀ η (k + 1)) ≤ f (gd_sequence f x₀ η k) := by
      intro k;
      convert gd_monotone hf_diff h_lip hη_pos.le hη_le using 1;
    exact Nat.recOn k ( by norm_num ) fun n ihn => by norm_num [ Finset.sum_range_succ ] at * ; nlinarith [ h_monotone n, h_monotone ( n + 1 ) ] ;
  -- By the telescoping inequality, we have $\sum_{i=0}^k (f(x_{i+1}) - f(x^*)) \leq \frac{1}{2\eta} (\|x_0 - x^*\|^2 - \|x_{k+1} - x^*\|^2)$.
  have h_telescope : ∀ k, ∑ i ∈ Finset.range (k + 1), (f (gd_sequence f x₀ η (i + 1)) - f x_star) ≤ (1 / (2 * η)) * (‖x₀ - x_star‖ ^ 2 - ‖gd_sequence f x₀ η (k + 1) - x_star‖ ^ 2) := by
    intro k
    induction' k with k ih;
    · convert gd_telescope hf_conv hf_diff h_lip h_min hη_pos hη_le using 1 ; norm_num [ Finset.sum_range_succ ];
      rfl;
    · have h_telescope_step : f (gd_sequence f x₀ η (k + 2)) - f x_star ≤ (1 / (2 * η)) * (‖gd_sequence f x₀ η (k + 1) - x_star‖ ^ 2 - ‖gd_sequence f x₀ η (k + 2) - x_star‖ ^ 2) := by
        convert gd_telescope hf_conv hf_diff h_lip h_min hη_pos hη_le using 1;
      rw [ Finset.sum_range_succ ] ; linarith!;
  refine' le_trans ( h_ind k ) ( le_trans ( mul_le_mul_of_nonneg_left ( h_telescope k ) ( by positivity ) ) _ );
  field_simp;
  exact sub_le_self _ ( sq_nonneg _ )
